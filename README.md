# Mistral-7B-Synthetic-Dataset
This repository is for the synthetic dataset generated by Mistral-7B Instruct Model. The dataset contains 1000 synthetic conversations between User and AI Assistant, covering a wide range of topics and tasks.


# Synthetic Data Creation via LLM 
## Yulin Hu & Yijun Liu

In this report, we detailed our methodology in using mistral-7b to generate synthetic data for training an AI assistant in broad topics. Due to the rate limits, we ran the model locally via Ollama. The maximum number of “turns” (the number of times that the user speaks and the AI responds) is 6, while the minimum is 1. 

1. Workflow:
Decide the number of conversations: NUM_CONVERSATIONS = 1000, per request in the assignment 

Generate NUM_CONVERSATIONS (1000) words (nouns) as conversation topics: get_n_topics()
The prompt used to generate the topics is the following: “Continue this comprehensive list of topics in conversations:\n 1. {word1}\n 2. {word2}\n 3. {word3}\n 4. {word4}\n 5. {word5}”
We obtain the word randomly from a self-created diverse dataset of words. This way, each time the function is called, it will have different examples. 
Since we have a token limit, the function will be called several times (and check for duplicates) to generate a comprehensive list of potential topics. 

Generate NUM_CONVERSATIONS (1000) starter sentences for the conversations: get_starter_sentence()
The prompt used is: “Give me some open-ended questions on the topic of {word}. Question:”
The function will then gather the first question generated and check for duplicates. In some cases, if after looping over the topic list but 1000 starter sentence requirement is still not met, we choose to start generating questions on a random topic from the list until 1000 starter sentences are satisfied. 

Update sys context and generate conversations: generate_conversation()
With the topic words and starter sentences, we feed both as inputs to the model. The function generate_conversation(topic, starter) is tuned to generate stable conversations with good quality so long as we provide the two parameters. The topic word is in the system prompt in this way: “<SYS> The following conversation is between a human and a helpful AI assistant. They are talking about {topic}. </SYS>”

Cleaning the dataset: find_substring_indices()
We clean the dataset using the following steps:
Limit the conversation to 6 turns by identifying the sixth time the assistant speaks by truncating the remaining part.
Add the context and starter sentence to complete the conversation.
Ensure that the conversation ends with the assistant’s response to make the conversation meaningful and complete.

Check conversation qualities: check_conversation_quality()
The prompt used is “Conversation:\n<SYS> The following conversation is between a human and a helpful AI assistant. They are talking about the weather </SYS>\nUSER: Hi, how are you?\nASSISTANT: Hi\nUSER: Great wethear\nASSISTANT: yeahhh\nComment: \nsimple sentences, grammar errors, and informal language, don't answer the user back\nRating: 1\n\n Conversation:\n <SYS> The following conversation is between a human and a helpful AI assistant. They are talking about USA </SYS>\n USER: Do you know where's the capital city of the USA?\n ASSISTANT: Yes, it's Washington D.C.. \n USER: Oh great! Thanks for that. Do you know the history of Washington D.C.?\n ASSISTANT: Washington, D.C., the capital city of the United States, has a rich and unique history. The location of the capital was selected by President George Washington in 1790. The area chosen was a square measuring 10 miles on each side, sitting on land donated by Maryland and Virginia. Comment: \n Rich content, free of grammar errors, variety of entities included in the assistant's response\n Rating: 5\n\n Conversation: {conversation}\n Rating:”.
It basically gives two examples, one good (5) and one bad (1) example, and the reasoning behind it. Then, we input the conversation to check the rating score. We believe that this is a good approach because it provides sufficient examples. 
In the implementation, any conversation with a score lower than 3 will be asked to regenerate the conversation based on the same starter sentence. If the starter sentence fails to generate quality conversations after 3 times, we move on and choose a different starter sentence.  


2. Topic & Instruction Variety 
We have adopted a framework to first generate the topics and then generate the starter sentences. Generating the topic list first allows for a great variety and the model is tuned to output topics in various fields based on the prompt. Additionally, the topics are checked for duplicates and dropped if any. For each topic, we choose to generate a question; if any topic fails to generate a question after retrying, in the end, we will randomly sample a topic to generate starter sentences. This way, the dataset will be diverse in capturing nearly all 1000 different topics, making it very diverse in terms of topics.


3. Uniqueness in Methods 
Compared to generating a list of tasks directly, we choose to first generate each topic (knowledge domain) that can potentially have greater coverage in terms of topics using several calls. Moreover, we implement special checks to obtain the first question in all the questions generated for a given topic. We believe that we have greater coverage of the topics compared to the starter code generating the instructions directly. 
Moreover, we added several safety layers to make sure that the text generated was what we wanted. In generating the questions, we add retries when return an empty list or no questions. In evaluating the quality score, we also add a retry when no numbers or illegal numbers (not within 1-5) are returned.  

4. Trade-offs 

4.1 Task vs Questions
We decide to generate a list of different topics first then move to questions. A good part about this approach is that it can cover a diverse domain. However, we are at the cost of not being able to implement some task instructions that are not question-based like “modify this HTML code: <html></html>”. It is extremely difficult to generate a consistent output for a task instruction on a given topic rather than a question that can be identified by a question mark. Nevertheless, we believe that most tasks are able to translate into question-based that have the potential to be included in the database. For instance, “Write me a report on math” is equivalent to “Can you write me a report on math?”. 

4.2 Few-shots vs One-shot
The template code uses a one-shot approach that generates the task directly. However, we take a different approach in generating the topic first, then the starter question, and finally the conversation itself. It might be cheaper to generate the task directly, however, we believe that our approach will have greater coverage in topic diversity. 

4.3 Tokens vs Calls 
We are limited to certain (and quite small) amounts of tokens. However, that means we need to make multiple calls that are possibly more expensive because of the duplicates. 

4.4 Cost vs Coverage & Quality
We can turn down the cost of calling the API by the following: 1) reduce the number of topics, making one topic cover several start sentences 2) remove re-calling the API when the start sentence is not a question by moving on to the next topic and therefore needs to have repeated topics and 3) remove the quality check. Nevertheless, 1 and 2 will reduce the coverage of the topics, and 3 will reduce the quality of data. 


5. Model Performance
The model frequently outputs an empty list for topic generation or duplicates for topics that are already in the list without enough examples. 
The model does not follow instructions well. It follows examples better than instructions. However, it makes the evaluation using this model difficult because long examples make it overfit and few examples make it difficult to train. 
The model cannot evaluate the conversation quality with metrics. It always returns 3.
We need to use different samples from the topic list to generate topics so it can reach 1000 different topics. 

6. Open-Source
The dataset is open to the public on GitHub under Apache License 2.0. Feel free to use or modify the data.
GitHub Repo: https://github.com/its-yulin/Mistral-7B-Synthetic-Dataset

